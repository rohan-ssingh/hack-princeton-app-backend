"""
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.documents.base import Blob
from langchain_core.tools import tool
from datetime import datetime, date
from collections import defaultdict

import requests

from pypdf import PdfReader
from uuid import uuid4
import zipfile
from tqdm import tqdm
import warnings

from pydantic import BaseModel
from typing import IO, Optional
from typing_extensions import List, TypedDict
import io

from dotenv import load_dotenv
import os
from pathlib import Path

from llm import llm, embeddings, image_parser, text_splitter, prompt

load_dotenv()

STORAGE_PATH = Path(
    os.getenv("STORAGE_PATH", str((Path(__file__).parent.parent / "uploads").resolve()))
).resolve()

# TODO: text splitting might be best done within textbook units

class Retrieval(TypedDict):
    question: str
    documents: List[Document]
    response: BaseModel | str


class RAGToolInput(BaseModel):
    question: str
    schema: Optional[BaseModel] = None
    date_range: Optional[List[str]] = None


class Storage:
    """Handles the storage and retrieval of document embeddings using FAISS & SQLAlchemy."""
    
    def __init__(self, path: str, from_path: bool = False):
        """Initialize the Storage class.

        Args:
            path (str): The path where the FAISS index file will be located.
            from_path (bool): If True, load the vector store from the specified path if it exists.
                              If False, will initialize a new vector store.
        """
        self.vector_store = None
        self.FAISS_INDEX_PATH = path

        # if a vector store already exists at path, and user specifies from_path, then load vector store from path rather than intializing a new one.
        if from_path:
            if os.path.exists(path):
                self.vector_store = FAISS.load_local(
                    path, embeddings, allow_dangerous_deserialization=True
                )
            else:
                warnings.warn(f"FAISS index file not found at {path}")

    def retrieve(self, query: str) -> List[Document]:
        """Retrieves documents from the vector store based on similarity to a query.

        Args:
            query (str): The query string to search for.

        Returns:
            List[Document]: A list of documents that match the query.
        """
        
        if not self.vector_store:
            raise ValueError("Vector store not initialized.")
        return self.vector_store.similarity_search(query)

    def rag(self, question: str, schema: Optional[BaseModel] = None, date_range: Optional[List[str]] = None):
        """Retrieve documents relevant to the input and generates a response. The documents here are state legislature records, including meeting transcripts, approved bills, and journals (daily notes of all legislature activities). Please use this to find information relevant to a given topic or issue. You can specify the date range of the outputs, to find more relevant information.

        Args:
            question (str): The question to retrieve context for.
            schema (Optional[BaseModel]): The schema for structured output. Defaults to None.
            date_range (Optional[List[str]]): A list containing the start and end date for filtering documents, in ["YYYY-MM-DD", "YYYY-MM-DD"] format.
        Returns:
            The response generated by the LLM based on the retrieved documents.
        """

        retrieved_docs = self.vector_store.similarity_search(question, k=15)

        # Filter by date range
        if date_range and len(date_range) == 2:
            start_date_str, end_date_str = date_range
            start_date = datetime.strptime(start_date_str, "%Y-%m-%d").date() if start_date_str else None
            end_date = datetime.strptime(end_date_str, "%Y-%m-%d").date() if end_date_str else None
            
            filtered_docs = []
            for doc in retrieved_docs:
                if 'journal_date' in doc.metadata and doc.metadata['journal_date']:
                    doc_date = doc.metadata['journal_date']
                    if isinstance(doc_date, str):
                        try:
                            doc_date = datetime.strptime(doc_date, "%Y-%m-%d").date()
                        except ValueError:
                            continue
                    
                    if (not start_date or (start_date <= doc_date)) and (not end_date or (doc_date <= end_date)):
                        filtered_docs.append(doc)
            retrieved_docs = filtered_docs
        
        docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)
        messages = prompt.invoke({"question": question, "context": docs_content})
        
        if schema:
            response = llm.with_structured_output(schema).invoke(messages)
        else:
            response = llm.invoke(messages).content
        
        return Retrieval(
            question=question,
            documents=retrieved_docs,
            response=response,
        )
        
    def add_documents(self, documents: List[Document]):
        """Adds documents to the vector store.
        
        Args:
            documents (List[Document]): A list of Document objects to be added.
        """
        if (
            not self.vector_store
        ):  # if vector store is not initialized, create a new one
            self.vector_store = FAISS.from_documents(
                documents=documents, embedding=embeddings
            )
        else:  # otherwise, add to existing vector store
            self.vector_store.add_documents(documents=documents)
        
        self.vector_store.save_local(self.FAISS_INDEX_PATH)

class PDF:
    def __init__(self, pdf_file: str | IO, storage: Storage, metadata: dict):
        """
        Initializes the Material class from PDF file.

        Args:
            pdf_file (str | IO): The path to the PDF file, or the file-like object, to load.
            storage (Storage): The Storage class for vector storage to associate with the material.
            metadata (dict): A dictionary containing metadata about the source file.
        """

        self.storage = storage
        document = self._load_documents(pdf_file)

        # Add the parsed metadata to the document itself.
        # This is useful if we want to see this info when retrieving docs.
        document.metadata.update(metadata)

        # split & store documents
        splits = text_splitter.split_documents([document])
        self.storage.add_documents(documents=splits)

    def _load_documents(self, pdf_file: str | IO) -> Document:
        """
        Loads documents from a PDF file.
        
        Args:
            pdf_file (str | IO): The path to the PDF file, or the file-like object, to load.
        
        Returns:
            Document: Document constructed from PDF content.
        """
        
        content = ""
        self.images = []
        
        reader = PdfReader(pdf_file)
        
        for page in tqdm(reader.pages, desc="Processing PDF pages"):
            content += page.extract_text() + "\n" # extract text from each page

        return Document(page_content=content, metadata={"source": pdf_file}, id=str(uuid4()))

def make_rag_tool(storage: Storage):
    @tool
    def rag(
        question: str,
        schema: Optional[BaseModel] = None,
        date_range: Optional[List[str]] = None,
    ):
        """Retrieve documents relevant to the input and generates a response. The documents here are state legislature records, including meeting transcripts, approved bills, and journals (daily notes of all legislature activities). Please use this to find information relevant to a given topic or issue. You can specify the date range of the outputs, to find more relevant information.

        Args:
            question (str): The question to retrieve context for.
            schema (Optional[BaseModel]): The schema for structured output. Defaults to None.
            date_range (Optional[List[str]]): A list containing the start and end date for filtering documents, in ["YYYY-MM-DD", "YYYY-MM-DD"] format.
        Returns:
            The response generated by the LLM based on the retrieved documents.
        """
        payload = RAGToolInput(
            question=question,
            schema=schema,
            date_range=date_range,
        )
        return storage.rag(**payload.dict())

    rag.__name__ = "rag"
    return rag
"""

import os
import json
import pickle
import warnings
from pathlib import Path
from datetime import datetime
from typing import Optional, IO
from typing_extensions import List, TypedDict

import faiss
from google.cloud import storage
from google.oauth2 import service_account
from pydantic import BaseModel
from pypdf import PdfReader
from tqdm import tqdm
from uuid import uuid4

from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.tools import tool
from langchain_openai import OpenAIEmbeddings

from llm import llm, embeddings, text_splitter, prompt


# ----------------------------------------
# GOOGLE SERVICE ACCOUNT CREDENTIALS
# ----------------------------------------
creds_json = os.getenv("GOOGLE_APPLICATION_CREDENTIALS_JSON")
if not creds_json:
    raise RuntimeError("Missing GOOGLE_APPLICATION_CREDENTIALS_JSON")

creds_dict = json.loads(creds_json)
google_credentials = service_account.Credentials.from_service_account_info(
    creds_dict,
    scopes=["https://www.googleapis.com/auth/cloud-platform"],
)


# ----------------------------------------
# GCS UTIL FUNCTIONS
# ----------------------------------------
def gcs_download(path: str) -> bytes:
    """Download a file from GCS into memory."""
    client = storage.Client(credentials=google_credentials)
    bucket_name = os.getenv("GCS_BUCKET_NAME")
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(path)

    print(f"[GCS] Downloading gs://{bucket_name}/{path}")
    data = blob.download_as_bytes()
    print(f"[GCS] Downloaded {len(data)/1_000_000:.2f} MB")
    return data


# ----------------------------------------
# LOAD FAISS INDEX + METADATA FROM GCS
# ----------------------------------------
def load_faiss_from_gcs():
    index_path = os.getenv("GCS_FAISS_INDEX_PATH")
    metadata_path = os.getenv("GCS_FAISS_PKL_PATH")

    if not index_path or not metadata_path:
        raise RuntimeError("Missing GCS_FAISS_INDEX_PATH or GCS_FAISS_PKL_PATH")

    raw_index = gcs_download(index_path)
    raw_meta = gcs_download(metadata_path)

    index = faiss.deserialize_index(raw_index)
    metadata = pickle.loads(raw_meta)

    print("[FAISS] Loaded index + metadata from GCS")

    vs = FAISS(
        embedding_function=embeddings,
        index=index,
        docstore=metadata["docstore"],
        index_to_docstore_id=metadata["index_to_docstore_id"],
    )

    return vs


# ====================================================
# Storage / RAG Logic — KEEPING YOUR ORIGINAL STRUCTURE
# ====================================================

class Retrieval(TypedDict):
    question: str
    documents: List[Document]
    response: BaseModel | str


class RAGToolInput(BaseModel):
    question: str
    schema: Optional[BaseModel] = None
    date_range: Optional[List[str]] = None


class Storage:
    """Your existing Storage class."""

    def __init__(self):
        self.vector_store = load_faiss_from_gcs()

    def retrieve(self, query: str):
        return self.vector_store.similarity_search(query)

    def rag(self, question: str, schema: Optional[BaseModel] = None, date_range: Optional[List[str]] = None):

        docs = self.vector_store.similarity_search(question, k=15)

        # Your existing date filtering logic
        if date_range and len(date_range) == 2:
            start, end = date_range
            start = datetime.strptime(start, "%Y-%m-%d").date() if start else None
            end = datetime.strptime(end, "%Y-%m-%d").date() if end else None

            filtered = []
            for doc in docs:
                dt = doc.metadata.get("journal_date")
                if isinstance(dt, str):
                    try:
                        dt = datetime.strptime(dt, "%Y-%m-%d").date()
                    except:
                        continue
                if dt and (not start or dt >= start) and (not end or dt <= end):
                    filtered.append(doc)
            docs = filtered

        ctx = "\n\n".join(d.page_content for d in docs)
        msgs = prompt.invoke({"question": question, "context": ctx})

        if schema:
            resp = llm.with_structured_output(schema).invoke(msgs)
        else:
            resp = llm.invoke(msgs).content

        return Retrieval(question=question, documents=docs, response=resp)

    def add_documents(self, docs: List[Document]):
        raise NotImplementedError("Not adding docs on Railway environment.")


# ===================================
# PDF document loader (unchanged)
# ===================================
class PDF:
    def __init__(self, pdf_file: str | IO, storage: Storage, metadata: dict):

        self.storage = storage
        document = self._load_documents(pdf_file)
        document.metadata.update(metadata)

        splits = text_splitter.split_documents([document])
        self.storage.add_documents(splits)

    def _load_documents(self, pdf_file: str | IO) -> Document:
        content = ""
        reader = PdfReader(pdf_file)
        for p in tqdm(reader.pages, desc="Processing PDF"):
            content += p.extract_text() + "\n"
        return Document(page_content=content, metadata={"source": pdf_file}, id=str(uuid4()))


# ========================================================
# MAKE RAG TOOL — SAME API EXPECTED BY chat_query.py
# ========================================================
def make_rag_tool(storage: Storage):
    @tool
    def rag(question: str, schema: Optional[BaseModel] = None, date_range: Optional[List[str]] = None):
        payload = RAGToolInput(question=question, schema=schema, date_range=date_range)
        return storage.rag(**payload.dict())

    rag.__name__ = "rag"
    return rag
